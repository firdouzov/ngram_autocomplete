# ğŸ‡¦ğŸ‡¿ Azerbaijani N-Gram Autocompleter

A real-time word prediction tool trained on Azerbaijani news text using N-Gram models (bigram and trigram). It suggests the next word based on the current sentence context using Laplace-smoothed probabilities.

Project URL: [Huggingface URL]((https://huggingface.co/spaces/firdouzov/autocompleter)

---

## âœ¨ Features

- ğŸ”¤ Bigram and trigram next-word prediction  
- ğŸ§  Laplace smoothing for unseen combinations  
- ğŸŒ Gradio interface for easy interaction  
- ğŸ“š Trained on Azerbaijani news corpora  
- ğŸ§ª Jupyter notebook for preprocessing  

---

## ğŸš€ Demo

> Input: `AzÉ™rbaycan respublikasÄ±`  
> Output: `AzÉ™rbaycan respublikasÄ± É™rÉ™b`

![Alt text](/autocomplete.png?raw=true "Screenshot Title")

Start typing any phrase related to Azerbaijani news, and the model will suggest the most probable next word.

âš ï¸ **Note**: Do not include trailing spaces after words. Example:  
âœ… `AzÉ™rbaycan respublikasÄ±`  
âŒ `AzÉ™rbaycan respublikasÄ± `

---

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/firdouzov/ngrams_autocomplete.git
cd az-ngrams-autocompleter
```

### Install dependencies

If you have a `requirements.txt`:

```bash
pip install -r requirements.txt
```

Or install manually:

```bash
pip install gradio
```

---

## ğŸ“ Project Structure

ngram-autocompleter/
â”œâ”€â”€ main.py                 # Main application file
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ ngram_preprocessor.ipynb # Data preprocessing notebook
â”œâ”€â”€ trigram.pkl.gz         # Trained trigram model (not uploaded)
â”œâ”€â”€ bigram.pkl.gz          # Trained bigram model (not uploaded)
â””â”€â”€ README.md              # This file

---
## ğŸŒ Deployment
Hugging Face Spaces
- This project is designed to be deployed on Hugging Face Spaces:
- 1. Create a new Space on Hugging Face
- 2. Upload all project files including model files
- 3. The app will automatically deploy with a permanent URL
   
---

## ğŸ§  How It Works

- If only **1 word** is entered â†’ uses **bigram model**  
- If **2+ words** are entered â†’ tries **trigram model**, falls back to **bigram** if needed  
- Uses Laplace smoothing for better prediction accuracy  
- Gradio provides the interactive web UI  

---

## ğŸ“Š Model Information

- The n-gram models were trained on the LocalDoc/news_azerbaijan dataset from Hugging Face. The preprocessing pipeline includes:
- * Text tokenization using NLTK
- * Lowercasing
- * Vocabulary filtering (threshold: 300 occurrences)
- * Unknown token handling (<UNK>)
- * Start/end tokens (<s>, </s>)

---
## Model Statistics
- * Vocabulary size: Filtered based on frequency threshold
- * N-gram orders: Unigram, Bigram, and Trigram
- * Smoothing: Laplace smoothing with k=1
---

## â–¶ï¸ Run the App

```bash
python main.py
```

This launches the Gradio web interface locally.

---

## ğŸ‘¤ Author

**Aydin Firdouzov**  
GitHub: [@firdouzov](https://github.com/firdouzov)

---

## ğŸ™Œ Acknowledgements
- Dataset: [LocalDoc/news_azerbaijan](https://huggingface.co/datasets/LocalDoc/news_azerbaijan)
- Built with [Gradio](https://gradio.app)  
- Inspired by traditional NLP N-Gram models  
